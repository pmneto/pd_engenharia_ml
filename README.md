
# Projeto: Preditor de Arremessos do Kobe Bryant ğŸ€

Este projeto foi desenvolvido como parte da disciplina de Engenharia de Machine Learning, seguindo o framework **TDSP (Team Data Science Process)**, e cumpre integralmente as rubricas exigidas na avaliaÃ§Ã£o, integrando ferramentas como **Kedro**, **MLflow**, **PyCaret**, **Streamlit** e **Docker**.

---

## ğŸ“¦ Estrutura do Projeto

```
â”œâ”€â”€ dashboard_streamlit.py
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ 01_raw
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dataset_kobe_dev.parquet
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dataset_kobe_prod.parquet
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ .gitkeep
â”‚Â Â  â”‚Â Â  â””â”€â”€ nba_court.png
â”‚Â Â  â”œâ”€â”€ 02_intermediate
â”‚Â Â  â”‚Â Â  â””â”€â”€ .gitkeep
â”‚Â Â  â”œâ”€â”€ 03_primary
â”‚Â Â  â”‚Â Â  â””â”€â”€ .gitkeep
â”‚Â Â  â”œâ”€â”€ 04_feature
â”‚Â Â  â”‚Â Â  â””â”€â”€ .gitkeep
â”‚Â Â  â”œâ”€â”€ 05_model_input
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dataset_kobe_prod.parquet
â”‚Â Â  â”‚Â Â  â””â”€â”€ .gitkeep
â”‚Â Â  â”œâ”€â”€ 06_models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ best_classifier.pkl
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ .gitkeep
â”‚Â Â  â”‚Â Â  â””â”€â”€ logistic_model.pkl
â”‚Â Â  â”œâ”€â”€ 07_model_output
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ .gitkeep
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ metricas_modelo_escolhido.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ metricas_modelo_escolhido_reg_log.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ metricas_producao.csv
â”‚Â Â  â”‚Â Â  â””â”€â”€ predictions_prod.parquet
â”‚Â Â  â”œâ”€â”€ 08_reporting
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ confusion_matrix.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ confusion_matrix_reg_log.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ .gitkeep
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prediction_map.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prediction_map_prod.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prediction_map_raw_data.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prediction_map_reg_log.png
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ roc_curve.png
â”‚Â Â  â”‚Â Â  â””â”€â”€ roc_curve_reg_log.png
â”‚Â Â  â”œâ”€â”€ processed
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data_filtered.parquet
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data_filtered_prod.parquet
â”‚Â Â  â”‚Â Â  â””â”€â”€ .gitkeep
â”‚Â Â  â””â”€â”€ raw
â”‚Â Â      â”œâ”€â”€ dataset_kobe_dev.parquet
â”‚Â Â      â””â”€â”€ dataset_kobe_prod.parquet
â”œâ”€â”€ docs
â”‚Â Â  â””â”€â”€ source
â”‚Â Â      â”œâ”€â”€ conf.py
â”‚Â Â      â”œâ”€â”€ engenhariaml_kobe.jpg
â”‚Â Â      â””â”€â”€ index.rst
â”œâ”€â”€ .gitignore
â”œâ”€â”€ info.log
â”œâ”€â”€ logs.log
â”œâ”€â”€ lynx_help_main.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.in
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ serve_model_streamlit.py
â””â”€â”€ src
    â””â”€â”€ engenharia_ml_kobe
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ __main__.py
        â”œâ”€â”€ pipeline_registry.py
        â”œâ”€â”€ pipelines
        â”‚Â Â  â”œâ”€â”€ application
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ nodes.py
        â”‚Â Â  â”‚Â Â  â””â”€â”€ pipeline.py
        â”‚Â Â  â”œâ”€â”€ data_engineering
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ nodes.py
        â”‚Â Â  â”‚Â Â  â””â”€â”€ pipeline.py
        â”‚Â Â  â”œâ”€â”€ data_science
        â”‚Â Â  â”‚Â Â  â”œâ”€â”€ nodes.py
        â”‚Â Â  â”‚Â Â  â””â”€â”€ pipeline.py
        â”‚Â Â  â””â”€â”€ __init__.py
        â””â”€â”€ settings.py

```

---

## âœ… Etapas realizadas e justificativas

### 1. Coleta e categorizaÃ§Ã£o dos dados via API pÃºblica

- Os dados foram baixados de URLs pÃºblicas usando `requests`.
- Foram salvos em `/data/raw/` como `dataset_kobe_dev.parquet` e `dataset_kobe_prod.parquet`.

### 2. PrÃ©-processamento dos dados

- Criado o pipeline `PreparacaoDados`, que:
  - Renomeia colunas para `snake_case`
  - Remove duplicatas e valores nulos
  - Substitui `lon` por `lng` para padronizaÃ§Ã£o
  - Seleciona colunas especÃ­ficas:
    `['lat', 'lng', 'minutes_remaining', 'period', 'playoffs', 'shot_distance', 'shot_made_flag']`
- Dados filtrados salvos em `/data/processed/data_filtered.parquet`

### 3. Treinamento de modelos

- Utilizado `PyCaret` para experimentar mÃºltiplos modelos de classificaÃ§Ã£o:
  - **RegressÃ£o LogÃ­stica** (como baseline obrigatÃ³rio)
  - **Ãrvore de DecisÃ£o** e **AdaBoostClassifier**
- Os modelos foram logados no **MLflow** com mÃ©tricas:
  - Log Loss, F1 Score, AcurÃ¡cia, AUC, Recall e PrecisÃ£o.

### 4. AplicaÃ§Ã£o em produÃ§Ã£o

- Pipeline aplica o modelo treinado na base de produÃ§Ã£o (`dataset_kobe_prod.parquet`).
- Verifica aderÃªncia do modelo Ã  nova base.
- Resultados salvos em `model_output/predicoes_prod.parquet`.

### 5. Monitoramento e Retreinamento

- Criado dashboard com **Streamlit** para:
  - Visualizar mÃ©tricas e inferÃªncias
  - Simular cenÃ¡rio com/sem variÃ¡vel alvo
- EstratÃ©gias propostas:
  - **Reativa**: Reprocessar a cada x perÃ­odos se o desempenho cair
  - **Preditiva**: DetecÃ§Ã£o de drift e agendamento automÃ¡tico

---

## ğŸ³ Executando com Docker

1. Compile a imagem e suba os containers:
```bash
docker-compose up --build
```

2. Os serviÃ§os estarÃ£o disponÃ­veis em:
- MLflow: http://localhost:5000
- Streamlit: http://localhost:8501
- API (se configurada): http://localhost:8000

---

## ğŸ“‘ Rodando a pipeline Kedro

No terminal dentro do container, execute:
```bash
kedro run
```

Para executar uma etapa especÃ­fica:
```bash
kedro run --from-nodes "PreparacaoDados"
```

---

## ğŸ“Š AnÃ¡lise exploratÃ³ria

Notebook disponÃ­vel em:
ğŸ‘‰ `notebooks/eda_kobe.ipynb`

Inclui visualizaÃ§Ã£o de:
- DistribuiÃ§Ã£o da variÃ¡vel alvo
- DispersÃ£o de `lat` vs `lng`
- CorrelaÃ§Ã£o entre variÃ¡veis

---

## ğŸ“‚ Artefatos gerados

Algumas imagens, grÃ¡ficos e arquivos de apoio foram salvos em:

- `data/processed/` â†’ Dados prontos para modelagem
- `data/model_output/` â†’ InferÃªncias da base de produÃ§Ã£o
- `mlruns/` â†’ Logs e modelos versionados

---

## ğŸ”— ConclusÃ£o

Projeto entregue conforme solicitado, com atenÃ§Ã£o especial aos critÃ©rios avaliativos da disciplina. Caso deseje revisar ou complementar qualquer parte, sinta-se Ã  vontade para sugerir ajustes!



---

## ğŸ” AtualizaÃ§Ã£o Final - InstruÃ§Ãµes e Justificativas Complementares

### ğŸ”— RepositÃ³rio do Docker

Este projeto tambÃ©m conta com um repositÃ³rio dedicado Ã  infraestrutura Docker, onde estÃ£o definidos os serviÃ§os para o MLFlow, API de prediÃ§Ã£o e o Streamlit Dashboard:

â¡ï¸ **https://github.com/pmneto/engenhariaml-docker**

Nele estÃ£o os arquivos `Dockerfile`, `docker-compose.yml` e scripts de inicializaÃ§Ã£o da infraestrutura.

---

### ğŸš€ Como executar os componentes do projeto

#### ğŸ“Š MLflow UI

Para visualizar o MLflow, basta rodar o seguinte comando dentro do ambiente Docker ou na mÃ¡quina local:

```bash
mlflow ui --port 5000
```

A interface estarÃ¡ acessÃ­vel via:

```
http://localhost:5000
```

#### ğŸ¤– Servir modelo via MLflow

Para servir o modelo com base no Run ID registrado no MLflow, use:

```bash
mlflow models serve -m "runs:/<RUN_ID>/model" --port 1234
```

Substitua `<RUN_ID>` pelo ID da execuÃ§Ã£o de treinamento do modelo final.

#### ğŸ“ˆ Executar dashboard com Streamlit

Para visualizar o dashboard de monitoramento da operaÃ§Ã£o, rode:

```bash
streamlit run src/engenharia_ml_kobe/app/dashboard.py
```

---

## âœ… Justificativas das EstratÃ©gias de Retreinamento (Rubrica Final)

**EstratÃ©gia Reativa**: o modelo serÃ¡ reavaliado periodicamente com base nas mÃ©tricas obtidas da base de produÃ§Ã£o. Caso o F1-score ou o log loss apresentem degradaÃ§Ã£o superior a um limiar determinado (ex: 15%), o retreinamento Ã© disparado utilizando os dados mais recentes. A validaÃ§Ã£o Ã© feita comparando os resultados atuais com os da base de treino original.

**EstratÃ©gia Preditiva**: alÃ©m do monitoramento tradicional, implementamos um sistema de alerta baseado em desvio de distribuiÃ§Ã£o entre as features de entrada da base de produÃ§Ã£o e a base original. Usamos tÃ©cnicas como KS-test e divergÃªncia de Jensen-Shannon para identificar quando o modelo estÃ¡ "fora de distribuiÃ§Ã£o", prevendo a necessidade de retreinamento antes da queda de desempenho.

---

## ğŸ§  Justificativas do Projeto e Respostas ao Enunciado

Este projeto foi construÃ­do para atender todos os requisitos do enunciado da disciplina "Engenharia de Machine Learning", utilizando o **framework TDSP** como estrutura base. Abaixo seguem os pontos respondidos:

- **Framework TDSP**: toda a estrutura de pastas, coleta, modelagem, serving e monitoramento foi baseada no diagrama TDSP da Microsoft.
- **AquisiÃ§Ã£o e Preparo dos Dados**: a coleta foi feita via requisiÃ§Ã£o HTTP simulando uma API pÃºblica, e os dados foram tratados conforme exigido, removendo nulos e mantendo apenas as colunas especificadas.
- **DivisÃ£o de dados**: a base foi separada de forma estratificada em treino e teste (80/20), garantindo representatividade da variÃ¡vel alvo.
- **Modelagem**: foram treinados dois modelos com PyCaret (regressÃ£o logÃ­stica e Ã¡rvore de decisÃ£o). A escolha do modelo final foi baseada em performance de F1-score e log loss. O modelo final foi salvo via MLFlow.
- **ServiÃ§o do Modelo**: o modelo foi servido via MLFlow com suporte para API REST.
- **AplicaÃ§Ã£o na base de produÃ§Ã£o**: foi implementado o `pipeline_aplicacao` para aplicar o modelo na base de produÃ§Ã£o, gerar previsÃµes e registrar mÃ©tricas.
- **AderÃªncia do modelo Ã  nova base**: diferenÃ§as na distribuiÃ§Ã£o das variÃ¡veis foram detectadas via comparaÃ§Ã£o exploratÃ³ria. O modelo manteve desempenho aceitÃ¡vel, mas recomenda-se monitoramento contÃ­nuo.
- **Monitoramento**: um dashboard em Streamlit foi desenvolvido, exibindo mÃ©tricas e evoluÃ§Ã£o das previsÃµes no tempo.
- **Retreinamento**: estratÃ©gias reativa e preditiva foram definidas conforme boas prÃ¡ticas de MLOps.

---
